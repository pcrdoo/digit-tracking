# -*- coding: utf-8 -*-
"""digit-tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZCfa4W4Z1adiNfTEwMmmxbZWlIKVafZh
"""

#@title PyDrive installation
!pip install -U -q PyDrive

#@title PyDrive authentication
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once in a notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from math import ceil, floor

import numpy as np
from skimage.filters import threshold_sauvola
from skimage.morphology import binary_dilation
from skimage.transform import rescale


# Defines a transformation that will be applied to both extracted digits and mnist dataset
# Output: background=black, binary
def transform_img(img, img_rows, img_cols, mnist=False):
    # Binarize
    thresh = threshold_sauvola(img, window_size=13, k=0.025, r=0.5)
    img = (img < thresh)

    [h, w] = img.shape

    # Pad
    if w > h:
        factor = img_cols / w
        img = rescale(img, factor, mode='constant', cval=0)
        diff = (img_rows - img.shape[0]) / 2

        img = np.pad(img, ((int(ceil(diff)), int(floor(diff))), (0, 0)),
                     'constant', constant_values=((0,)))
    else:
        factor = img_rows / h
        img = rescale(img, factor, mode='constant', cval=0)
        diff = (img_cols - img.shape[1]) / 2

        img = np.pad(img, ((0, 0), (int(ceil(diff)), int(floor(diff)))),
                     'constant', constant_values=((0,)))

    # Binarize again
    if mnist:
        ret = (img > 0.4).astype(int)
    else:
        ret = binary_dilation(img)
        ret = ret.astype(int)

    return ret

import random

import cv2
import numpy as np
from skimage.measure import label
from tensorflow.python import keras
from tensorflow.python.keras import backend as K

#from utils import transform_img


class MnistDataset:

    def pad(self, imgs):
        # List of padded images
        padded = []
        batch_sz = imgs.shape[0]
        for i in range(batch_sz):
            if i % 1000 == 0:
                print('Padding {}'.format(i))

            # Binarize
            img = imgs[i, :, :]
            bin = (img > 0).astype(int)
            wh_o = np.where(bin == 1)
            wh = (wh_o[1], wh_o[0])
            t = np.dstack(wh)[0]

            # Find bounding rect
            x, y, w, h = cv2.boundingRect(t)

            # Crop
            cropped = img[y:y+h, x:x+w]
            cropped2 = 1.0 - cropped

            # Transform
            trans = transform_img(cropped2, 28, 28, True)
            padded.append(trans)

        # Return the whole batch
        return np.stack(padded)

    # Very rough hack that has a significant impact on classification of ones
    def fix_one(self, img):
        # Binarize and find components
        ret, thresh = cv2.threshold(img, 1, 255, 0)
        lab, max_label = label(thresh, return_num=True)

        # Find number_of_pixels / minarearect_size ratio
        rat = 0
        largest_comp = 0
        for i in range(1, max_label+1):
            component_o = np.where(lab == i)
            component = (component_o[1], component_o[0])
            filled_px = component[0].size

            t = np.dstack(component)[0]
            rect = cv2.minAreaRect(t)
            box = np.int0(cv2.boxPoints(rect))
            ar = rect[1][0] * rect[1][1]

            if ar == 0:
                continue

            if filled_px > largest_comp:
                largest_comp = filled_px
                rat = filled_px / ar

        # If that ratio is large we can safely assume that this is a "one-line" one
        # Augment it to create a "two-line" one
        if rat > 0.9:
            x = np.min(box[:, 0])
            y = np.min(box[:, 1])
            center = (x, y)
            M = cv2.getRotationMatrix2D(center, -30, 1)
            img2 = cv2.warpAffine(
                img, M, img.shape, borderMode=cv2.BORDER_REPLICATE)
            mask = np.zeros(img.shape)
            mask[y:y+10, :] = 1
            img2 *= mask
            return np.maximum(img, img2)
        return None

    def __init__(self, augmentation):
        # Input image dimensions
        img_rows, img_cols = (28, 28)
        self.num_classes = 10

        # Split
        (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

        # Use floats
        x_train = x_train.astype('float32')
        x_test = x_test.astype('float32')

        # Fix ones (only on train set, this will make final eval worse)
        nws = []
        all = []
        idxs = []
        for i in range(x_train.shape[0]):
            if y_train[i] == 1:
                idxs.append(i)
                all.append(x_train[i])
                img = x_train[i]
                nw = self.fix_one(img)
                if nw is not None:
                    nws.append(nw)

        # Sample and combine
        old_cnt = len(all) - len(nws)
        randIndex = random.sample(range(len(all)), old_cnt)
        randIndex.sort()
        sampled = [all[i] for i in randIndex]
        nws.extend(sampled)

        for i in range(len(idxs)):
            x_train[idxs[i]] = nws[i]

        # Normalize
        x_train /= 255
        x_test /= 255

        # Augment
        if augmentation == 'pad':
            x_train = self.pad(x_train)
            x_test = self.pad(x_test)

        # Unsqueeze
        if K.image_data_format() == 'channels_first':
            x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
            x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
            self.shape = (1, img_rows, img_cols)
        else:
            x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
            self.shape = (img_rows, img_cols, 1)

        # Convert class vectors to binary class matrices
        y_train = keras.utils.to_categorical(y_train, self.num_classes)
        y_test = keras.utils.to_categorical(y_test, self.num_classes)

        # Save
        self.x_train, self.x_test = x_train, x_test
        self.y_train, self.y_test = y_train, y_test

from __future__ import print_function

from tensorflow.python import keras
from tensorflow.python.keras.layers import (Conv2D, Dense, Dropout, Flatten,
                                            MaxPooling2D)
from tensorflow.python.keras.models import Sequential

# from dataset import MnistDataset

# Create the dataset
dataset = MnistDataset(augmentation='pad')

# Params
batch_size = 128
epochs = 12

# Define the model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=dataset.shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(dataset.num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(dataset.x_train, dataset.y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(dataset.x_test, dataset.y_test))

# Evaluate
score = model.evaluate(dataset.x_test, dataset.y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

#@title Save model to drive
# My folder: https://drive.google.com/drive/folders/1Kgb1I51hACQJ17vxWUktaKOHNaMAzjaO
label = 'normal' # LABEL
folder_id = '1Kgb1I51hACQJ17vxWUktaKOHNaMAzjaO'

from datetime import datetime
import pytz
dt = datetime.now(pytz.timezone('Europe/Belgrade'))
timestamp = datetime.strftime(dt, "%m-%d_%H:%M:%S")

model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
json = drive.CreateFile({
    'title': 'model_{}_{}.json'.format(timestamp, label), 
    'parents': [{'kind': 'drive#fileLink', 
                 'id': folder_id}]
})
json.SetContentFile('model.json')
json.Upload()


model.save_weights("model.h5")
json = drive.CreateFile({
    'title': 'model_{}_{}.h5'.format(timestamp, label), 
    'parents': [{'kind': 'drive#fileLink', 
                 'id': folder_id}]
})
json.SetContentFile('model.h5')
json.Upload()

print('Saved')